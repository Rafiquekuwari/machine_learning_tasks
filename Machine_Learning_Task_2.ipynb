{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPufDAgengo19haAmztCAMk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rafiquekuwari/machine_learning_tasks/blob/main/Machine_Learning_Task_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K93E5g3Jq3GO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from scipy.stats import skew"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "data = fetch_california_housing()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['Target'] = data.target\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "Lrfs5fOarGHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Exploration\n",
        "print(df.info())\n",
        "print(df.describe())"
      ],
      "metadata": {
        "id": "SIEfcaGNrKgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isna().any().sum())  # This should print the total count of columns with missing values\n"
      ],
      "metadata": {
        "id": "F1h8yXvmECrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values\n",
        "print(\"Before Handling Missing Values:\")\n",
        "print(df.isnull().sum())\n",
        "df_before_missing = df.copy()\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "df[df.columns] = imputer.fit_transform(df)\n",
        "print(\"After Handling Missing Values:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"Missing values handled.\")"
      ],
      "metadata": {
        "id": "f3eid4_5BV4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display before and after summary\n",
        "print(\"Summary Before Handling Missing Values:\")\n",
        "print(df_before_missing.describe())\n",
        "print(\"Summary After Handling Missing Values:\")\n",
        "print(df.describe())\n"
      ],
      "metadata": {
        "id": "Cj-WCaWVDuR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers using IQR Method\n",
        "print(\"Before Removing Outliers:\")\n",
        "print(df.describe())\n",
        "Q1 = df.quantile(0.25)\n",
        "Q3 = df.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "print(\"After Removing Outliers:\")\n",
        "print(df.describe())\n",
        "print(\"Outliers removed using IQR method.\")"
      ],
      "metadata": {
        "id": "vSLXjN4lBaJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = df.corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()\n",
        "print(\"Correlation analysis completed.\")"
      ],
      "metadata": {
        "id": "9HtF-EKFETBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Selection: Removing highly correlated features\n",
        "print(\"Before Removing Highly Correlated Features:\")\n",
        "print(df.columns)\n",
        "threshold = 0.9\n",
        "correlated_features = set()\n",
        "for i in range(len(corr_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(corr_matrix.iloc[i, j]) > threshold:\n",
        "            correlated_features.add(corr_matrix.columns[i])\n",
        "df = df.drop(columns=correlated_features)\n",
        "print(\"After Removing Highly Correlated Features:\")\n",
        "print(df.columns)\n",
        "print(\"Highly correlated features removed.\")\n"
      ],
      "metadata": {
        "id": "GZhNTr2xEZu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Before Skewness Handling:\")\n",
        "skewed_features = df.drop(columns=['Target']).apply(lambda x: skew(x)).sort_values(ascending=False)\n",
        "print(skewed_features)\n",
        "for feature in skewed_features.index:\n",
        "    if abs(skewed_features[feature]) > 0.75:\n",
        "        df[feature] = np.log1p(df[feature])\n",
        "print(\"After Skewness Handling:\")\n",
        "skewed_features = df.drop(columns=['Target']).apply(lambda x: skew(x)).sort_values(ascending=False)\n",
        "print(skewed_features)"
      ],
      "metadata": {
        "id": "xTO1Uu2NEkxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Features considered for log transformation:\")\n",
        "print(skewed_features[skewed_features.abs() > 0.75])\n"
      ],
      "metadata": {
        "id": "tGJLgTpAE5F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-Test Split\n",
        "X = df.drop(columns=['Target'])\n",
        "y = df['Target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Train-test split completed.\")"
      ],
      "metadata": {
        "id": "Gt9MZwtsriue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Scaling\n",
        "print(\"Before Scaling:\")\n",
        "print(pd.DataFrame(X_train).describe())\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "print(\"After Scaling:\")\n",
        "print(pd.DataFrame(X_train_scaled).describe())\n",
        "\n",
        "print(\"Data scaling completed.\")\n"
      ],
      "metadata": {
        "id": "DwgR7VSSrmyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering: Adding Polynomial Features\n",
        "print(\"Before Polynomial Feature Addition:\")\n",
        "print(X_train.shape)\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_train_poly = poly.fit_transform(X_train_scaled)\n",
        "X_test_poly = poly.transform(X_test_scaled)\n",
        "print(\"After Polynomial Feature Addition:\")\n",
        "print(X_train_poly.shape)\n",
        "print(\"Polynomial features added.\")"
      ],
      "metadata": {
        "id": "4MPcBibpGTku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training & Evaluation\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    end_time = time.time()\n",
        "\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    return mae, rmse, r2, elapsed_time, y_pred"
      ],
      "metadata": {
        "id": "YVjx9QO1rqa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train and evaluate the model\n",
        "mae, rmse, r2, elapsed_time, y_pred = evaluate_model(model, X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "cross_val = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2').mean()\n",
        "\n",
        "# Store results\n",
        "results = {\n",
        "    'MAE': round(mae, 6),\n",
        "    'RMSE': round(float(rmse), 6),\n",
        "    'R² Score': round(r2, 6),\n",
        "    'Cross-Val R²': round(float(cross_val), 6),\n",
        "    'Time (s)': round(elapsed_time, 6)\n",
        "}\n",
        "predictions = y_pred\n",
        "\n",
        "# Print or log results\n",
        "print(f\"Multiple Linear Regression\")\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "fRBA_BfidPoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_scaled.shape)\n"
      ],
      "metadata": {
        "id": "VIeLXDwdlSNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    'Simple Linear Regression': LinearRegression(),\n",
        "    'Multiple Linear Regression': LinearRegression(),\n",
        "    'Ridge Regression': Ridge(alpha=1.0),\n",
        "    'Lasso Regression': Lasso(alpha=0.1),\n",
        "    'Polynomial Regression': LinearRegression(),\n",
        "    'SVR': SVR(kernel='rbf'),\n",
        "    'Decision Tree': DecisionTreeRegressor(),\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100),\n",
        "}"
      ],
      "metadata": {
        "id": "wHQYCkSjrtjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "predictions = {}\n",
        "cross_val_results = {}\n",
        "\n",
        "X_train_simple = X_train_scaled[:, 0].reshape(-1, 1)  # Select only the first feature\n",
        "X_test_simple = X_test_scaled[:, 0].reshape(-1, 1)\n",
        "\n",
        "for name, model in models.items():\n",
        "    if name == 'Polynomial Regression':\n",
        "        mae, rmse, r2, elapsed_time, y_pred = evaluate_model(model, X_train_poly, X_test_poly, y_train, y_test)\n",
        "        cross_val = cross_val_score(model, X_train_poly, y_train, cv=5, scoring='r2').mean()\n",
        "    elif name == 'Simple Linear Regression':\n",
        "        mae, rmse, r2, elapsed_time, y_pred = evaluate_model(model, X_train_simple, X_test_simple, y_train, y_test)\n",
        "        cross_val = cross_val_score(model, X_train_simple, y_train, cv=5, scoring='r2').mean()\n",
        "    else:\n",
        "        mae, rmse, r2, elapsed_time, y_pred = evaluate_model(model, X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "        cross_val = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2').mean()\n",
        "\n",
        "    results[name] = {\n",
        "        'MAE': mae,\n",
        "        'RMSE': rmse,\n",
        "        'R² Score': r2,\n",
        "        'Cross-Val R²': cross_val,\n",
        "        'Time (s)': elapsed_time\n",
        "    }\n",
        "    predictions[name] = y_pred\n"
      ],
      "metadata": {
        "id": "2XDE_Y9jrw71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert results to DataFrame & Display\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df)\n",
        "\n"
      ],
      "metadata": {
        "id": "46WUTTHXsCIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(data=results_df, x=results_df.index, y='R² Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Model Performance Comparison\")\n",
        "plt.show()\n",
        "print(\"Visualization completed.\")\n"
      ],
      "metadata": {
        "id": "5hX1jvdSsMO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "for name, y_pred in predictions.items():\n",
        "    residuals = y_test - y_pred\n",
        "    sm.qqplot(residuals, line='45')\n",
        "    plt.title(f\"Residual Plot for {name}\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "CxEubcOYUCpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conclusion\n",
        "best_model = results_df['R² Score'].idxmax()\n",
        "print(f\"The best performing model is: {best_model} with an R² Score of {results_df.loc[best_model, 'R² Score']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SF6xWMliWv4q",
        "outputId": "a6cbc43d-ded4-43fb-df0e-5f0e966a9bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best performing model is: Random Forest with an R² Score of 0.6961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BrgpIIuf-PRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git push origin main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytPxOi11-1T6",
        "outputId": "805ebda6-6baf-4b3c-871c-7d3f34d7bacf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Everything up-to-date\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/Colab_Notebooks/Machine_Learning_Task_2.ipynb /content/\n"
      ],
      "metadata": {
        "id": "ojVOPoNH-7eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXZpscWn-9tK",
        "outputId": "6e739d75-ab91-4b53-cf68-9be4d420ca25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 28K\n",
            "drwx------ 6 root root 4.0K Mar 23 12:24 drive\n",
            "-rw------- 1 root root  14K Mar 23 12:43 Machine_Learning_Task_2.ipynb\n",
            "-rw-r--r-- 1 root root  200 Mar 23 12:04 README.md\n",
            "drwxr-xr-x 1 root root 4.0K Mar 20 13:31 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add Machine_Learning_Task_2.ipynb\n",
        "!git commit -m \"Added Machine Learning Task 2 notebook\"\n",
        "!git push origin main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQqQKZTp-_2Z",
        "outputId": "9a294c93-cc6e-47c5-95fd-7319e1e42fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main 01196ff] Added Machine Learning Task 2 notebook\n",
            " 1 file changed, 1 insertion(+), 1 deletion(-)\n",
            " rewrite Machine_Learning_Task_2.ipynb (97%)\n",
            "Enumerating objects: 5, done.\n",
            "Counting objects: 100% (5/5), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (3/3), done.\n",
            "Writing objects: 100% (3/3), 680 bytes | 680.00 KiB/s, done.\n",
            "Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "To https://github.com/MohammedRafiqueKuwari/machine_Learning_Tasks.git\n",
            "   2db1cb6..01196ff  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"Machine_Learning_Task_2.ipynb\", \"r\") as f:\n",
        "    json.load(f)  # This will throw an error if the JSON is invalid\n"
      ],
      "metadata": {
        "id": "PEy5WD6jCgqb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}